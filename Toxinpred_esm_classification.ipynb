{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMRbwna7TW6P",
        "outputId": "2844b65b-379d-4588-eaca-abd5df933243",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: fair-esm in /home/molecular16/.local/lib/python3.11/site-packages (2.0.0)\n",
            "Requirement already satisfied: torch in /home/molecular16/.local/lib/python3.11/site-packages (2.7.1)\n",
            "Requirement already satisfied: filelock in /home/molecular16/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /home/molecular16/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /home/molecular16/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /home/molecular16/.local/lib/python3.11/site-packages (from torch) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from triton==3.3.1->torch) (68.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install fair-esm torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFuIz6HIUZrw",
        "outputId": "1dd0768d-b735-4686-8ab8-639b2fb188bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFLw5b7bR926"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    matthews_corrcoef, cohen_kappa_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import esm\n",
        "\n",
        "\n",
        "# Load ESM model\n",
        "esm_model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "esm_model.eval()\n",
        "\n",
        "# Dataset Class\n",
        "class PeptideDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        df = pd.read_csv(csv_file)\n",
        "        self.sequences = df['sequence'].tolist()\n",
        "        self.labels = df['toxin'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# ESM Embedding Extractor\n",
        "@torch.no_grad()\n",
        "def extract_esm_embeddings(sequences):\n",
        "    # Convert sequences to uppercase to match ESM alphabet expectations\n",
        "    sequences_upper = [seq.upper() for seq in sequences]\n",
        "    data = [(\"seq\", seq) for seq in sequences_upper] # Use uppercase sequences\n",
        "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "    results = esm_model(batch_tokens, repr_layers=[6], return_contacts=False)\n",
        "    token_representations = results[\"representations\"][6]\n",
        "\n",
        "    # Mean pooling (ignore padding and BOS/EOS)\n",
        "    embeddings = []\n",
        "    for i, seq in enumerate(sequences_upper): # Use uppercase sequences here as well\n",
        "        emb = token_representations[i, 1:len(seq)+1].mean(0)\n",
        "        embeddings.append(emb)\n",
        "    return torch.stack(embeddings)\n",
        "\n",
        "\n",
        "# TCN Block\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super().__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride,\n",
        "                      padding=padding, dilation=dilation),\n",
        "            Chomp1d(padding),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride,\n",
        "                      padding=padding, dilation=dilation),\n",
        "            Chomp1d(padding),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return F.relu(out + res)\n",
        "\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    def __init__(self, input_size, num_channels, kernel_size=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for i in range(len(num_channels)):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = input_size if i == 0 else num_channels[i - 1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers.append(\n",
        "                TemporalBlock(in_channels, out_channels, kernel_size, stride=1,\n",
        "                              dilation=dilation_size, padding=(kernel_size - 1) * dilation_size,\n",
        "                              dropout=dropout)\n",
        "            )\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "# Adaptive Feature Fusion\n",
        "class AdaptiveFusion(nn.Module):\n",
        "    def __init__(self, esm_dim, tcn_dim):\n",
        "        super().__init__()\n",
        "        self.fc_esm = nn.Linear(esm_dim, esm_dim)\n",
        "        self.fc_tcn = nn.Linear(tcn_dim, esm_dim)\n",
        "        self.gate = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, esm_feat, tcn_feat):\n",
        "        esm_proj = self.fc_esm(esm_feat)\n",
        "        tcn_proj = self.fc_tcn(tcn_feat)\n",
        "        gate = self.gate(esm_proj + tcn_proj)\n",
        "        return gate * esm_proj + (1 - gate) * tcn_proj\n",
        "\n",
        "# Complete Classifier Model\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    # Changed tcn_input from 20 to 21 to match the one-hot encoding dimension\n",
        "    def __init__(self, esm_dim=320, tcn_input=21, tcn_channels=[64, 128], lstm_hidden=128, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.tcn = TCN(tcn_input, tcn_channels)\n",
        "        self.fusion = AdaptiveFusion(esm_dim, tcn_channels[-1])\n",
        "        self.lstm = nn.LSTM(input_size=esm_dim, hidden_size=lstm_hidden,\n",
        "                            num_layers=5, batch_first=True, dropout=0.3)\n",
        "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
        "\n",
        "    def forward(self, esm_feats, onehot_seqs):\n",
        "        tcn_out = self.tcn(onehot_seqs.permute(0, 2, 1))\n",
        "        tcn_summary = torch.mean(tcn_out, dim=2)  # Global pooling\n",
        "        fused = self.fusion(esm_feats, tcn_summary)\n",
        "        lstm_input = fused.unsqueeze(1).repeat(1, 10, 1)  # Repeat to simulate sequence\n",
        "        lstm_out, _ = self.lstm(lstm_input)\n",
        "        out = self.classifier(lstm_out[:, -1])\n",
        "        return out\n",
        "\n",
        "# Utility Functions\n",
        "# Also update the sequence_to_onehot function to use uppercase sequences for consistency\n",
        "def sequence_to_onehot(sequences, max_len=100):\n",
        "    amino_acids = 'RHKDESTNQCUGPAVILMFYW'\n",
        "    aa_to_idx = {aa: i for i, aa in enumerate(amino_acids)}\n",
        "    onehot = torch.zeros(len(sequences), max_len, len(amino_acids))\n",
        "    for i, seq in enumerate(sequences):\n",
        "        # Convert sequence to uppercase before processing\n",
        "        seq_upper = seq.upper()\n",
        "        for j, aa in enumerate(seq_upper[:max_len]):\n",
        "            if aa in aa_to_idx:\n",
        "                onehot[i, j, aa_to_idx[aa]] = 1.0\n",
        "    return onehot\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20):\n",
        "    history = {\"train_loss\": [], \"val_loss\": []}\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for sequences, labels in tqdm(train_loader):\n",
        "            esm_feats = extract_esm_embeddings(sequences)\n",
        "            onehot_seqs = sequence_to_onehot(sequences).float()\n",
        "            labels = torch.tensor(labels)\n",
        "\n",
        "            outputs = model(esm_feats, onehot_seqs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for sequences, labels in val_loader:\n",
        "                esm_feats = extract_esm_embeddings(sequences)\n",
        "                onehot_seqs = sequence_to_onehot(sequences).float()\n",
        "                labels = torch.tensor(labels)\n",
        "\n",
        "                outputs = model(esm_feats, onehot_seqs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "    return history\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in data_loader:\n",
        "            esm_feats = extract_esm_embeddings(sequences)\n",
        "            onehot_seqs = sequence_to_onehot(sequences).float()\n",
        "            outputs = model(esm_feats, onehot_seqs)\n",
        "            preds = torch.argmax(outputs, dim=1).numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
        "    print(\"Precision:\", precision_score(all_labels, all_preds))\n",
        "    print(\"Recall:\", recall_score(all_labels, all_preds))\n",
        "    print(\"F1-score:\", f1_score(all_labels, all_preds))\n",
        "    print(\"MCC:\", matthews_corrcoef(all_labels, all_preds))\n",
        "    print(\"Cohenâ€™s Kappa:\", cohen_kappa_score(all_labels, all_preds))\n",
        "\n",
        "\n",
        "# Plotting\n",
        "def plot_history(history):\n",
        "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training/Validation Loss\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "0HX1wNZuTFCZ",
        "outputId": "a96c7f65-ff97-40e9-c90c-61ca056bf89b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'PeptideDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4029550485.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Main Execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeptideDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"toxinpred_augmented_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mval_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mval_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PeptideDataset' is not defined"
          ]
        }
      ],
      "source": [
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = PeptideDataset(\"toxinpred_augmented_data.csv\")\n",
        "    val_split = 0.2\n",
        "    n_val = int(len(dataset) * val_split)\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [len(dataset) - n_val, n_val])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "    model = MultimodalClassifier()\n",
        "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    history = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50)\n",
        "    plot_history(history)\n",
        "\n",
        "    print(\"Train Set Evaluation:\")\n",
        "    evaluate_model(model, train_loader)\n",
        "\n",
        "    print(\"Validation Set Evaluation:\")\n",
        "    evaluate_model(model, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "Gp58-o_-gnan",
        "outputId": "c8e44d52-226d-4a49-ded9-878322b55853"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3207357796.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# After training and evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Confusion Matrix - Training Set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Confusion Matrix - Validation Set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_confusion_matrix(model, data_loader, title):\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in data_loader:\n",
        "            esm_feats = extract_esm_embeddings(sequences)\n",
        "            onehot_seqs = sequence_to_onehot(sequences).float()\n",
        "            outputs = model(esm_feats, onehot_seqs)\n",
        "            preds = torch.argmax(outputs, dim=1).numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# After training and evaluation\n",
        "plot_confusion_matrix(model, train_loader, \"Confusion Matrix - Training Set\")\n",
        "plot_confusion_matrix(model, val_loader, \"Confusion Matrix - Validation Set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "SvYodzRbZNDK",
        "outputId": "07ed9d95-a2bd-46e3-a691-1b19b2569858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "toxin\n",
            "0    27590\n",
            "1    27590\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as df\n",
        "df = pd.read_csv(\"toxinpred_augmented_data.csv\")\n",
        "toxin_counts = df['toxin'].value_counts()\n",
        "print(toxin_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzUaaTdvH_UH"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'toxinpred_peptide_classifier.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oThFaWgXIZ72",
        "outputId": "3b0d57a5-e790-4267-af4d-cc4a2a071f8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Predictions: tensor([1, 0])\n"
          ]
        }
      ],
      "source": [
        "# To open the trained model, you would load the state dictionary back into a model instance\n",
        "# First, instantiate a new model with the same architecture\n",
        "loaded_model = MultimodalClassifier()\n",
        "\n",
        "# Then, load the saved state dictionary\n",
        "loaded_model.load_state_dict(torch.load('toxinpred_peptide_classifier.pth'))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# You can now use loaded_model for inference on new data.\n",
        "# Example (assuming you have new_sequences):\n",
        "new_sequences = [\"ARGLAKL\", \"AAVVRR\"]\n",
        "new_esm_feats = extract_esm_embeddings(new_sequences)\n",
        "new_onehot_seqs = sequence_to_onehot(new_sequences).float()\n",
        "with torch.no_grad():\n",
        "     predictions = loaded_model(new_esm_feats, new_onehot_seqs)\n",
        "     predicted_classes = torch.argmax(predictions, dim=1)\n",
        "     print(\"Predictions:\", predicted_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCLP61tsUZr1"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow\n",
        "#import tensorflow as tf\n",
        "#print(tf.config.list_physical_devices(\"GPU\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN_GCnpLIwny",
        "outputId": "c9d4136a-c663-4bc6-be17-8ecb2dbf74ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: lime in /home/molecular16/.local/lib/python3.11/site-packages (0.2.0.1)\n",
            "Requirement already satisfied: matplotlib in /home/molecular16/anaconda3/lib/python3.11/site-packages (from lime) (3.7.2)\n",
            "Requirement already satisfied: numpy in /home/molecular16/anaconda3/lib/python3.11/site-packages (from lime) (1.24.3)\n",
            "Requirement already satisfied: scipy in /home/molecular16/anaconda3/lib/python3.11/site-packages (from lime) (1.11.1)\n",
            "Requirement already satisfied: tqdm in /home/molecular16/anaconda3/lib/python3.11/site-packages (from lime) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from lime) (1.3.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from lime) (0.20.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-image>=0.12->lime) (3.1)\n",
            "Requirement already satisfied: pillow>=9.0.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-image>=0.12->lime) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-image>=0.12->lime) (2.31.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-image>=0.12->lime) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-image>=0.12->lime) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-image>=0.12->lime) (23.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-image>=0.12->lime) (0.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.18->lime) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.18->lime) (2.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from matplotlib->lime) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from matplotlib->lime) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from matplotlib->lime) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from matplotlib->lime) (1.4.4)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from matplotlib->lime) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7uJ5xoQUZr1",
        "outputId": "dfbe192d-dc96-47b3-be5f-91eb71406a46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Predictions: tensor([1, 0])\n"
          ]
        }
      ],
      "source": [
        "# To open the trained model, you would load the state dictionary back into a model instance\n",
        "# First, instantiate a new model with the same architecture\n",
        "model = MultimodalClassifier()\n",
        "\n",
        "# Then, load the saved state dictionary\n",
        "model.load_state_dict(torch.load('toxinpred_peptide_classifier.pth'))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# You can now use loaded_model for inference on new data.\n",
        "# Example (assuming you have new_sequences):\n",
        "new_sequences = [\"ARGLAKL\", \"AAVVRR\"]\n",
        "new_esm_feats = extract_esm_embeddings(new_sequences)\n",
        "new_onehot_seqs = sequence_to_onehot(new_sequences).float()\n",
        "with torch.no_grad():\n",
        "     predictions = model(new_esm_feats, new_onehot_seqs)\n",
        "     predicted_classes = torch.argmax(predictions, dim=1)\n",
        "     print(\"Predictions:\", predicted_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cnyxvaVZI1-O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "51c28742-5464-406a-f035-c3317a12afa9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'lime'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3506036632.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlime_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lime'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# === Load dataset ===\n",
        "df = pd.read_csv(\"toxinpred_augmented_data.csv\")\n",
        "df['sequence'] = df['sequence'].astype(str)\n",
        "df['label'] = df['toxin']\n",
        "\n",
        "# === Tokenization into k-mers ===\n",
        "def seq_to_kmers(seq, k=3):\n",
        "    return ' '.join([seq[i:i + k] for i in range(len(seq) - k + 1)])\n",
        "\n",
        "k = 3\n",
        "df['kmers'] = df['sequence'].apply(lambda seq: seq_to_kmers(seq, k))\n",
        "\n",
        "# === Helper to reconstruct full sequence from k-mers ===\n",
        "def kmers_to_seq(kmer_str, k=3):\n",
        "    kmers = kmer_str.split()\n",
        "    if not kmers:\n",
        "        return ''\n",
        "    return kmers[0] + ''.join([k[-1] for k in kmers[1:]])\n",
        "\n",
        "# === Prediction wrapper for LIME (accepts k-mer strings) ===\n",
        "class_names = ['non-toxin', 'toxin']\n",
        "\n",
        "def lime_predict_kmers(kmer_texts):\n",
        "    # Convert k-mer text back to raw sequences\n",
        "    sequences = [kmers_to_seq(text, k) for text in kmer_texts]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Filter out invalid inputs\n",
        "        valid_sequences = [seq for seq in sequences if isinstance(seq, str) and len(seq) > 0]\n",
        "        if not valid_sequences:\n",
        "            return np.array([[0.5, 0.5]] * len(sequences))\n",
        "\n",
        "        # ESM and one-hot encodings\n",
        "        esm_feats = extract_esm_embeddings(valid_sequences)  # tensor [B, D]\n",
        "        onehot_seqs = sequence_to_onehot(valid_sequences).float()  # tensor [B, L, 4]\n",
        "        outputs = model(esm_feats, onehot_seqs)\n",
        "        probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "        # Reassign to full array\n",
        "        full_probs = np.zeros((len(sequences), len(class_names)))\n",
        "        valid_idx = 0\n",
        "        for i, seq in enumerate(sequences):\n",
        "            if isinstance(seq, str) and len(seq) > 0:\n",
        "                full_probs[i] = probs[valid_idx]\n",
        "                valid_idx += 1\n",
        "            else:\n",
        "                full_probs[i] = np.array([0.5, 0.5])\n",
        "        return full_probs\n",
        "\n",
        "# === LIME explainer with whitespace as k-mer separator ===\n",
        "explainer = LimeTextExplainer(class_names=class_names, split_expression='\\\\s+')\n",
        "\n",
        "# === Select top-N high-confidence toxic sequences ===\n",
        "raw_sequences = df['sequence'].tolist()\n",
        "labels = df['label'].tolist()\n",
        "kmer_texts = df['kmers'].tolist()\n",
        "\n",
        "probs = lime_predict_kmers(kmer_texts)\n",
        "high_conf_ids = np.where((np.array(labels) == 1) & (probs[:, 1] > 0.8))[0]\n",
        "selected_ids = high_conf_ids[:10]\n",
        "\n",
        "# === Run LIME explanation ===\n",
        "for i, idx in enumerate(selected_ids):\n",
        "    kmer_input = kmer_texts[idx]\n",
        "    original_seq = df.iloc[idx]['sequence']\n",
        "\n",
        "    explanation = explainer.explain_instance(kmer_input, lime_predict_kmers, num_features=10, labels=[1], num_samples=1000)\n",
        "\n",
        "    print(f\"\\nðŸ§¬ Sequence {i+1}: {original_seq}\")\n",
        "    print(\"Top influential k-mers (toward toxin):\")\n",
        "    for token, weight in explanation.as_list(label=1):\n",
        "        print(f\"  {token}: {weight:.4f}\")\n",
        "\n",
        "    # Highlight influential regions in the original sequence\n",
        "    highlighted = original_seq\n",
        "    sorted_kmers = sorted(explanation.as_list(label=1), key=lambda x: abs(x[1]), reverse=True)\n",
        "    for kmer, _ in sorted_kmers:\n",
        "        highlighted = highlighted.replace(kmer, f\"<{kmer}>\")\n",
        "    print(\"Highlighted:\", highlighted)\n",
        "\n",
        "    try:\n",
        "        explanation.show_in_notebook()\n",
        "        fig = explanation.as_pyplot_figure(label=1)\n",
        "        plt.title(f\"LIME Explanation for Sequence {i+1}\")\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate plot for sequence {i+1}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mja-Z1z-Jkq-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "a21b9c23-88ee-4323-f4f5-762ea16c4bb6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'lime'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3839465082.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlime_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lime'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# === Load dataset ===\n",
        "df = pd.read_csv(\"toxinpred_augmented_data.csv\")\n",
        "df['sequence'] = df['sequence'].astype(str)\n",
        "df['label'] = df['toxin']\n",
        "\n",
        "# === Tokenization into k-mers ===\n",
        "def seq_to_kmers(seq, k=3):\n",
        "    return ' '.join([seq[i:i + k] for i in range(len(seq) - k + 1)])\n",
        "\n",
        "k = 3\n",
        "df['kmers'] = df['sequence'].apply(lambda seq: seq_to_kmers(seq, k))\n",
        "\n",
        "# === Helper to reconstruct full sequence from k-mers ===\n",
        "def kmers_to_seq(kmer_str, k=3):\n",
        "    kmers = kmer_str.split()\n",
        "    if not kmers:\n",
        "        return ''\n",
        "    return kmers[0] + ''.join([k[-1] for k in kmers[1:]])\n",
        "\n",
        "# === Prediction wrapper for LIME (accepts k-mer strings) ===\n",
        "class_names = ['toxin', 'non-toxin']\n",
        "\n",
        "def lime_predict_kmers(kmer_texts):\n",
        "    # Convert k-mer text back to raw sequences\n",
        "    sequences = [kmers_to_seq(text, k) for text in kmer_texts]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Filter out invalid inputs\n",
        "        valid_sequences = [seq for seq in sequences if isinstance(seq, str) and len(seq) > 0]\n",
        "        if not valid_sequences:\n",
        "            return np.array([[0.5, 0.5]] * len(sequences))\n",
        "\n",
        "        # ESM and one-hot encodings\n",
        "        esm_feats = extract_esm_embeddings(valid_sequences)  # tensor [B, D]\n",
        "        onehot_seqs = sequence_to_onehot(valid_sequences).float()  # tensor [B, L, 4]\n",
        "        outputs = model(esm_feats, onehot_seqs)\n",
        "        probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "        # Reassign to full array\n",
        "        full_probs = np.zeros((len(sequences), len(class_names)))\n",
        "        valid_idx = 0\n",
        "        for i, seq in enumerate(sequences):\n",
        "            if isinstance(seq, str) and len(seq) > 0:\n",
        "                full_probs[i] = probs[valid_idx]\n",
        "                valid_idx += 1\n",
        "            else:\n",
        "                full_probs[i] = np.array([0.5, 0.5])\n",
        "        return full_probs\n",
        "\n",
        "# === LIME explainer with whitespace as k-mer separator ===\n",
        "explainer = LimeTextExplainer(class_names=class_names, split_expression='\\\\s+')\n",
        "\n",
        "# === Select top-N high-confidence toxic sequences ===\n",
        "raw_sequences = df['sequence'].tolist()\n",
        "labels = df['label'].tolist()\n",
        "kmer_texts = df['kmers'].tolist()\n",
        "\n",
        "probs = lime_predict_kmers(kmer_texts)\n",
        "high_conf_ids = np.where((np.array(labels) == 0) & (probs[:, 1] > 0.8))[0]\n",
        "selected_ids = high_conf_ids[:10]\n",
        "\n",
        "# === Run LIME explanation ===\n",
        "for i, idx in enumerate(selected_ids):\n",
        "    kmer_input = kmer_texts[idx]\n",
        "    original_seq = df.iloc[idx]['sequence']\n",
        "\n",
        "    explanation = explainer.explain_instance(kmer_input, lime_predict_kmers, num_features=10, labels=[0], num_samples=1000)\n",
        "\n",
        "    print(f\"\\nðŸ§¬ Sequence {i+1}: {original_seq}\")\n",
        "    print(\"Top influential k-mers (toward non-toxin):\")\n",
        "    for token, weight in explanation.as_list(label=0):\n",
        "        print(f\"  {token}: {weight:.4f}\")\n",
        "\n",
        "    # Highlight influential regions in the original sequence\n",
        "    highlighted = original_seq\n",
        "    sorted_kmers = sorted(explanation.as_list(label=0), key=lambda x: abs(x[1]), reverse=True)\n",
        "    for kmer, _ in sorted_kmers:\n",
        "        highlighted = highlighted.replace(kmer, f\"<{kmer}>\")\n",
        "    print(\"Highlighted:\", highlighted)\n",
        "\n",
        "    try:\n",
        "        explanation.show_in_notebook()\n",
        "        fig = explanation.as_pyplot_figure(label=0)\n",
        "        plt.title(f\"LIME Explanation for Sequence {i+1}\")\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate plot for sequence {i+1}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DasuhnbNKDFT",
        "outputId": "c73268a8-516a-489b-c96f-26f18b0ef3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: anchor-exp in /home/molecular16/.local/lib/python3.11/site-packages (0.0.2.0)\n",
            "Requirement already satisfied: numpy in /home/molecular16/anaconda3/lib/python3.11/site-packages (from anchor-exp) (1.24.3)\n",
            "Requirement already satisfied: scipy in /home/molecular16/anaconda3/lib/python3.11/site-packages (from anchor-exp) (1.11.1)\n",
            "Requirement already satisfied: spacy in /home/molecular16/.local/lib/python3.11/site-packages (from anchor-exp) (3.8.7)\n",
            "Requirement already satisfied: lime in /home/molecular16/.local/lib/python3.11/site-packages (from anchor-exp) (0.2.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from anchor-exp) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.22->anchor-exp) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.22->anchor-exp) (2.2.0)\n",
            "Requirement already satisfied: matplotlib in /home/molecular16/anaconda3/lib/python3.11/site-packages (from lime->anchor-exp) (3.7.2)\n",
            "Requirement already satisfied: tqdm in /home/molecular16/anaconda3/lib/python3.11/site-packages (from lime->anchor-exp) (4.65.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from lime->anchor-exp) (0.20.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/molecular16/.local/lib/python3.11/site-packages (from spacy->anchor-exp) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/molecular16/.local/lib/python3.11/site-packages (from spacy->anchor-exp) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/molecular16/.local/lib/python3.11/site-packages (from spacy->anchor-exp) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/molecular16/.local/lib/python3.11/site-packages (from spacy->anchor-exp) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/molecular16/.local/lib/python3.11/site-packages (from spacy->anchor-exp) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/molecular16/.local/lib/python3.11/site-packages (from spacy->anchor-exp) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/molecular16/.local/lib/python3.11/site-packages (from spacy->anchor-exp) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/molecular16/.local/lib/python3.11/site-packages (from spacy->anchor-exp) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/molecular16/.local/lib/python3.11/site-packages (from spacy->anchor-exp) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/molecular16/.local/lib/python3.11/site-packages (from spacy->anchor-exp) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/molecular16/.local/lib/python3.11/site-packages (from spacy->anchor-exp) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from spacy->anchor-exp) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from spacy->anchor-exp) (1.10.8)\n",
            "Requirement already satisfied: jinja2 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from spacy->anchor-exp) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /home/molecular16/anaconda3/lib/python3.11/site-packages (from spacy->anchor-exp) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from spacy->anchor-exp) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/molecular16/.local/lib/python3.11/site-packages (from spacy->anchor-exp) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /home/molecular16/.local/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->anchor-exp) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /home/molecular16/.local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->anchor-exp) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy->anchor-exp) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy->anchor-exp) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy->anchor-exp) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy->anchor-exp) (2023.7.22)\n",
            "Requirement already satisfied: networkx>=2.8 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-image>=0.12->lime->anchor-exp) (3.1)\n",
            "Requirement already satisfied: pillow>=9.0.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-image>=0.12->lime->anchor-exp) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-image>=0.12->lime->anchor-exp) (2.31.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-image>=0.12->lime->anchor-exp) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-image>=0.12->lime->anchor-exp) (1.4.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from scikit-image>=0.12->lime->anchor-exp) (0.2)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /home/molecular16/.local/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy->anchor-exp) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/molecular16/.local/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy->anchor-exp) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy->anchor-exp) (8.0.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /home/molecular16/.local/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy->anchor-exp) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /home/molecular16/.local/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy->anchor-exp) (14.0.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/molecular16/.local/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy->anchor-exp) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy->anchor-exp) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from jinja2->spacy->anchor-exp) (2.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from matplotlib->lime->anchor-exp) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from matplotlib->lime->anchor-exp) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from matplotlib->lime->anchor-exp) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from matplotlib->lime->anchor-exp) (1.4.4)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from matplotlib->lime->anchor-exp) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from matplotlib->lime->anchor-exp) (2.8.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /home/molecular16/.local/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->anchor-exp) (1.2.1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: six>=1.5 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->lime->anchor-exp) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->anchor-exp) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->anchor-exp) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/molecular16/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->anchor-exp) (0.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install anchor-exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "szMAyvHTKHHe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "c35807f8-5fa9-4f42-f773-8dbcb028a63f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'anchor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1448656104.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0manchor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manchor_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'anchor'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from anchor import anchor_text\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# === Load dataset ===\n",
        "df = pd.read_csv(\"toxinpred_augmented_data.csv\")\n",
        "df['sequence'] = df['sequence'].astype(str)\n",
        "df['label'] = df['toxin']\n",
        "\n",
        "# === Convert sequence to k-mers ===\n",
        "def seq_to_kmers(seq, k=3):\n",
        "    return ' '.join([seq[i:i + k] for i in range(len(seq) - k + 1)])\n",
        "\n",
        "# === Convert k-mers back to sequence ===\n",
        "def kmers_to_seq(kmer_str, k=3):\n",
        "    kmers = kmer_str.split()\n",
        "    if not kmers: return ''\n",
        "    return kmers[0] + ''.join([k[-1] for k in kmers[1:]])\n",
        "\n",
        "k = 3\n",
        "df['kmers'] = df['sequence'].apply(lambda seq: seq_to_kmers(seq, k))\n",
        "kmer_texts = df['kmers'].tolist()\n",
        "raw_sequences = df['sequence'].tolist()\n",
        "labels = df['label'].tolist()\n",
        "\n",
        "# === Prediction using ESM + one-hot ===\n",
        "def predict_fn_esm(sequences):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_sequences = [s for s in sequences if isinstance(s, str) and len(s) > 0]\n",
        "        if not valid_sequences:\n",
        "            return np.array([[0.5, 0.5]] * len(sequences))\n",
        "\n",
        "        esm_feats = extract_esm_embeddings(valid_sequences)\n",
        "        onehot = sequence_to_onehot(valid_sequences).float()\n",
        "        outputs = model(esm_feats, onehot)\n",
        "        probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "        full_probs = np.zeros((len(sequences), 2))\n",
        "        valid_idx = 0\n",
        "        for i, seq in enumerate(sequences):\n",
        "            if isinstance(seq, str) and len(seq) > 0:\n",
        "                full_probs[i] = probs[valid_idx]\n",
        "                valid_idx += 1\n",
        "            else:\n",
        "                full_probs[i] = np.array([0.5, 0.5])\n",
        "        return full_probs\n",
        "\n",
        "# === Wrapper for AnchorText (returns class predictions) ===\n",
        "def predict_probs_kmers_anchor(kmer_seqs):\n",
        "    recovered_seqs = [kmers_to_seq(text, k=k) for text in kmer_seqs]\n",
        "    return predict_fn_esm(recovered_seqs)\n",
        "\n",
        "def predict_class_kmers_anchor(kmer_seqs):\n",
        "    probs = predict_probs_kmers_anchor(kmer_seqs)\n",
        "    return np.argmax(probs, axis=1)\n",
        "\n",
        "# === Dummy tokenizer for AnchorText ===\n",
        "class DummyToken:\n",
        "    def __init__(self, text, idx):\n",
        "        self.text = text\n",
        "        self.idx = idx\n",
        "\n",
        "class DummyTokenizer:\n",
        "    def __call__(self, text):\n",
        "        tokens = text.split()\n",
        "        return [DummyToken(token, i) for i, token in enumerate(tokens)]\n",
        "\n",
        "# === Find top high-confidence toxic predictions ===\n",
        "probs = predict_fn_esm(raw_sequences)\n",
        "high_conf_ids = np.where((np.array(labels) == 1) & (probs[:, 1] > 0.8))[0]\n",
        "selected_ids = high_conf_ids[:10]\n",
        "\n",
        "# === Create AnchorText explainer ===\n",
        "class_names = ['non-toxin', 'toxin']\n",
        "explainer = anchor_text.AnchorText(nlp=DummyTokenizer(), class_names=class_names)\n",
        "\n",
        "# === Explain selected instances ===\n",
        "for i, idx in enumerate(selected_ids):\n",
        "    print(f\"\\nðŸ§¬ Explaining Sequence {i+1} (Index {idx})\")\n",
        "    print(\"Original Sequence:\", raw_sequences[idx])\n",
        "\n",
        "    explanation = explainer.explain_instance(\n",
        "        kmer_texts[idx],\n",
        "        classifier_fn=predict_class_kmers_anchor,\n",
        "        threshold=0.95\n",
        "    )\n",
        "\n",
        "    print(\"\\nðŸ” Anchor Explanation:\")\n",
        "    print('Anchor (if these k-mers present â†’ toxin):', ' AND '.join(explanation.names()))\n",
        "    print('Precision:', explanation.precision())\n",
        "    print('Coverage:', explanation.coverage())\n",
        "    explanation.show_in_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FHjBvI64KcpP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "66295adc-7ba3-44c8-a42e-016d74989daa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'anchor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2006246664.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0manchor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manchor_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'anchor'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from anchor import anchor_text\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# === Load dataset ===\n",
        "df = pd.read_csv(\"toxinpred_augmented_data.csv\")\n",
        "df['sequence'] = df['sequence'].astype(str)\n",
        "df['label'] = df['toxin']\n",
        "\n",
        "# === Convert sequence to k-mers ===\n",
        "def seq_to_kmers(seq, k=3):\n",
        "    return ' '.join([seq[i:i + k] for i in range(len(seq) - k + 1)])\n",
        "\n",
        "# === Convert k-mers back to sequence ===\n",
        "def kmers_to_seq(kmer_str, k=3):\n",
        "    kmers = kmer_str.split()\n",
        "    if not kmers: return ''\n",
        "    return kmers[0] + ''.join([k[-1] for k in kmers[1:]])\n",
        "\n",
        "k = 3\n",
        "df['kmers'] = df['sequence'].apply(lambda seq: seq_to_kmers(seq, k))\n",
        "kmer_texts = df['kmers'].tolist()\n",
        "raw_sequences = df['sequence'].tolist()\n",
        "labels = df['label'].tolist()\n",
        "\n",
        "# === Prediction using ESM + one-hot ===\n",
        "def predict_fn_esm(sequences):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_sequences = [s for s in sequences if isinstance(s, str) and len(s) > 0]\n",
        "        if not valid_sequences:\n",
        "            return np.array([[0.5, 0.5]] * len(sequences))\n",
        "\n",
        "        esm_feats = extract_esm_embeddings(valid_sequences)\n",
        "        onehot = sequence_to_onehot(valid_sequences).float()\n",
        "        outputs = model(esm_feats, onehot)\n",
        "        probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "        full_probs = np.zeros((len(sequences), 2))\n",
        "        valid_idx = 0\n",
        "        for i, seq in enumerate(sequences):\n",
        "            if isinstance(seq, str) and len(seq) > 0:\n",
        "                full_probs[i] = probs[valid_idx]\n",
        "                valid_idx += 1\n",
        "            else:\n",
        "                full_probs[i] = np.array([0.5, 0.5])\n",
        "        return full_probs\n",
        "\n",
        "# === Wrapper for AnchorText (returns class predictions) ===\n",
        "def predict_probs_kmers_anchor(kmer_seqs):\n",
        "    recovered_seqs = [kmers_to_seq(text, k=k) for text in kmer_seqs]\n",
        "    return predict_fn_esm(recovered_seqs)\n",
        "\n",
        "def predict_class_kmers_anchor(kmer_seqs):\n",
        "    probs = predict_probs_kmers_anchor(kmer_seqs)\n",
        "    return np.argmax(probs, axis=1)\n",
        "\n",
        "# === Dummy tokenizer for AnchorText ===\n",
        "class DummyToken:\n",
        "    def __init__(self, text, idx):\n",
        "        self.text = text\n",
        "        self.idx = idx\n",
        "\n",
        "class DummyTokenizer:\n",
        "    def __call__(self, text):\n",
        "        tokens = text.split()\n",
        "        return [DummyToken(token, i) for i, token in enumerate(tokens)]\n",
        "\n",
        "# === Find top high-confidence toxic predictions ===\n",
        "probs = predict_fn_esm(raw_sequences)\n",
        "high_conf_ids = np.where((np.array(labels) == 0) & (probs[:, 1] > 0.8))[0]\n",
        "selected_ids = high_conf_ids[:10]\n",
        "\n",
        "# === Create AnchorText explainer ===\n",
        "class_names = ['non-toxin', 'toxin']\n",
        "explainer = anchor_text.AnchorText(nlp=DummyTokenizer(), class_names=class_names)\n",
        "\n",
        "# === Explain selected instances ===\n",
        "for i, idx in enumerate(selected_ids):\n",
        "    print(f\"\\nðŸ§¬ Explaining Sequence {i+1} (Index {idx})\")\n",
        "    print(\"Original Sequence:\", raw_sequences[idx])\n",
        "\n",
        "    explanation = explainer.explain_instance(\n",
        "        kmer_texts[idx],\n",
        "        classifier_fn=predict_class_kmers_anchor,\n",
        "        threshold=0.95\n",
        "    )\n",
        "\n",
        "    print(\"\\nðŸ” Anchor Explanation:\")\n",
        "    print('Anchor (if these k-mers present â†’ non-toxin):', ' AND '.join(explanation.names()))\n",
        "    print('Precision:', explanation.precision())\n",
        "    print('Coverage:', explanation.coverage())\n",
        "    explanation.show_in_notebook()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT-uuVdbUZr3"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"toxinpred_augmented_data.csv\")\n",
        "df['sequence'] = df['sequence'].astype(str).tolist()\n",
        "df['label'] = df['toxin'].tolist()\n",
        "\n",
        "# Define a fixed max_len for consistent feature representation\n",
        "max_len_for_shap = 512\n",
        "\n",
        "# Corrected tokenizer functions\n",
        "def sequence_to_features(seq, max_len):\n",
        "    \"\"\"Convert sequence to flattened one-hot features with padding\"\"\"\n",
        "    # Assuming amino acid sequences (RHKDESTNQCUGPAVILMFYW)\n",
        "    bases = 'RHKDESTNQCUGPAVILMFYW'\n",
        "    onehot = np.zeros((max_len, len(bases)))\n",
        "\n",
        "    for i in range(min(len(seq), max_len)):  # Ensure we don't go beyond sequence length\n",
        "        base = seq[i]\n",
        "        if base in bases:\n",
        "            onehot[i, bases.index(base)] = 1\n",
        "    return onehot.flatten()\n",
        "\n",
        "def features_to_sequence(flattened_onehot, max_len):\n",
        "    \"\"\"Convert flattened one-hot vector back to sequence\"\"\"\n",
        "    bases = 'RHKDESTNQCUGPAVILMFYW'\n",
        "    onehot = flattened_onehot.reshape((max_len, len(bases)))\n",
        "    sequence = []\n",
        "    for pos in onehot:\n",
        "        if np.any(pos != 0):\n",
        "            sequence.append(bases[np.argmax(pos)])\n",
        "    return ''.join(sequence)\n",
        "\n",
        "# Convert sequences to feature space\n",
        "X = np.array([sequence_to_features(seq, max_len=max_len_for_shap) for seq in sequences])\n",
        "\n",
        "# Use larger background for better SHAP approximation\n",
        "background_indices = np.random.choice(len(X), size=min(100, len(X)), replace=False)\n",
        "X_background = X[background_indices]\n",
        "\n",
        "# Corrected prediction function\n",
        "def shap_predict_fn(feature_vectors):\n",
        "    # Convert feature vectors back to sequences\n",
        "    reconstructed_seqs = []\n",
        "    for fv in feature_vectors:\n",
        "        try:\n",
        "            seq = features_to_sequence(fv, max_len=max_len_for_shap)\n",
        "            if len(seq) > 0:\n",
        "                reconstructed_seqs.append(seq)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    if not reconstructed_seqs:\n",
        "        return np.zeros((len(feature_vectors), 2)) + 0.5  # Neutral prediction for invalid inputs\n",
        "\n",
        "    # Get model predictions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        esm_feats = extract_esm_embeddings(reconstructed_seqs)\n",
        "        onehot_seqs = sequence_to_onehot(reconstructed_seqs).float()\n",
        "        outputs = model(esm_feats, onehot_seqs)\n",
        "        probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "    # Match output size to input size (some sequences might have been filtered)\n",
        "    final_probs = np.zeros((len(feature_vectors), 2)) + 0.5\n",
        "    valid_count = 0\n",
        "    for i, fv in enumerate(feature_vectors):\n",
        "        try:\n",
        "            if features_to_sequence(fv, max_len_for_shap):\n",
        "                final_probs[i] = probs[valid_count]\n",
        "                valid_count += 1\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return final_probs\n",
        "\n",
        "# Initialize explainer\n",
        "#explainer = shap.KernelExplainer(shap_predict_fn, X_background)\n",
        "\n",
        "# Initialize explainer with the k-mer prediction function\n",
        "explainer = shap.KernelExplainer(shap_predict_ids, X_padded_kmer_ids[background_indices])\n",
        "\n",
        "# Select examples to explain\n",
        "predicted_probs = shap_predict_fn(X)  # Use feature vectors, not raw sequences\n",
        "selected_ids = np.where((np.array(labels) == 1) & (predicted_probs[:, 1] > 0.8))[0][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ce04669aa0ff49ad8f93dd882936b6c3"
          ]
        },
        "id": "6VNoIwhVUZr3",
        "outputId": "e547f119-eabc-4993-a6a1-8e46df7b681b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum k-mer sequence length: 36\n",
            "Explaining sequence ID: 4415\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce04669aa0ff49ad8f93dd882936b6c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import shap\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"toxinpred_augmented_data.csv\")\n",
        "df['sequence'] = df['sequence'].astype(str).tolist()\n",
        "df['label'] = df['toxin'].tolist()\n",
        "\n",
        "# Function to tokenize peptide sequences into k-mers\n",
        "def seq_to_kmers(seq, k=3):\n",
        "    return ' '.join([seq[i:i+k] for i in range(len(seq) - k + 1)])\n",
        "\n",
        "k = 3\n",
        "df['kmers'] = df['sequence'].apply(lambda seq: seq_to_kmers(seq, k))\n",
        "kmer_texts = df['kmers'].tolist()\n",
        "\n",
        "# Function to convert k-mer text back to full sequence\n",
        "def kmers_to_seq(kmer_str, k=3):\n",
        "    kmers = kmer_str.split()\n",
        "    if not kmers:\n",
        "        return ''\n",
        "    return kmers[0] + ''.join([kmer[-1] for kmer in kmers[1:]])\n",
        "\n",
        "# 1. Build k-mer vocabulary and mapping to integer IDs\n",
        "all_kmers = [kmer for text in kmer_texts for kmer in text.split()]\n",
        "kmer_vocab = sorted(list(set(all_kmers)))\n",
        "# Add a padding token to the vocabulary\n",
        "padding_token = \"<PAD>\"\n",
        "if padding_token not in kmer_vocab:\n",
        "    kmer_vocab.append(padding_token)\n",
        "kmer_to_id = {kmer: i for i, kmer in enumerate(kmer_vocab)}\n",
        "id_to_kmer = {i: kmer for kmer, i in kmer_to_id.items()}\n",
        "padding_id = kmer_to_id[padding_token]\n",
        "\n",
        "\n",
        "# 2. Convert k-mer text sequences to integer ID sequences\n",
        "kmer_id_sequences = []\n",
        "for text in kmer_texts:\n",
        "    kmer_ids = [kmer_to_id[kmer] for kmer in text.split()]\n",
        "    kmer_id_sequences.append(kmer_ids)\n",
        "\n",
        "# Determine maximum k-mer sequence length for padding\n",
        "max_kmer_len = max(len(ids) for ids in kmer_id_sequences)\n",
        "# After defining max_kmer_len, add a check:\n",
        "print(f\"Maximum k-mer sequence length: {max_kmer_len}\")\n",
        "if max_kmer_len > 53:  # Or whatever your limit is\n",
        "    max_kmer_len = 53  # Set a safe upper limit\n",
        "    print(f\"Adjusted max k-mer sequence length to: {max_kmer_len}\")\n",
        "\n",
        "# Then pad with this adjusted length\n",
        "X_padded_kmer_ids = np.full((len(kmer_id_sequences), max_kmer_len), padding_id, dtype=int)\n",
        "for i, ids in enumerate(kmer_id_sequences):\n",
        "    length_to_copy = min(len(ids), max_kmer_len)\n",
        "    X_padded_kmer_ids[i, :length_to_copy] = ids[:length_to_copy]\n",
        "\n",
        "\n",
        "# 3. Pad integer ID sequences\n",
        "X_padded_kmer_ids = np.full((len(kmer_id_sequences), max_kmer_len), padding_id, dtype=int)\n",
        "for i, ids in enumerate(kmer_id_sequences):\n",
        "    X_padded_kmer_ids[i, :len(ids)] = ids\n",
        "\n",
        "\n",
        "# Define the missing predict_fn function\n",
        "def predict_fn(sequences):\n",
        "    \"\"\"\n",
        "    Takes list of sequences and returns predicted probabilities as numpy array (n_samples, 2)\n",
        "    Handles both ESM embeddings and one-hot encoded sequences\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Generate ESM embeddings\n",
        "        esm_feats = extract_esm_embeddings(sequences)  # Shape: [batch_size, esm_dim]\n",
        "\n",
        "        # Generate one-hot encoded sequences\n",
        "        onehot_seqs = sequence_to_onehot(sequences).float()  # Shape: [batch_size, seq_len, 4]\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = model(esm_feats, onehot_seqs)  # Now passing both required arguments\n",
        "        probs = torch.softmax(outputs, dim=1).numpy()\n",
        "\n",
        "    return probs  # Shape (n_samples, 2)\n",
        "\n",
        "\n",
        "# Now the shap_predict_ids function will work since predict_fn is defined\n",
        "def shap_predict_ids(padded_kmer_id_arrays):\n",
        "    original_seqs = []\n",
        "    for padded_row_ids in padded_kmer_id_arrays:\n",
        "        kmers_without_padding = [id_to_kmer[id] for id in padded_row_ids if id != padding_id]\n",
        "        kmer_text_str = ' '.join(kmers_without_padding)\n",
        "        original_seqs.append(kmers_to_seq(kmer_text_str, k))\n",
        "    return predict_fn(original_seqs)\n",
        "\n",
        "\n",
        "# Fix the labels reference (was using 'labels' which wasn't defined)\n",
        "labels = df['label'].tolist()\n",
        "sequences = df['sequence'].tolist()\n",
        "\n",
        "# Select high-confidence toxic predictions (using the now-defined predict_fn)\n",
        "probs = predict_fn(sequences)\n",
        "high_conf_ids = np.where((np.array(labels) == 1) & (probs[:, 1] > 0.8))[0]\n",
        "selected_ids = high_conf_ids[:10]\n",
        "\n",
        "# Explain each selected instance\n",
        "\n",
        "for idx in selected_ids:\n",
        "    if idx >= len(X_padded_kmer_ids):\n",
        "        continue  # Skip if index is out of bounds\n",
        "    print(f\"Explaining sequence ID: {idx}\")\n",
        "    # Get the explanation for this instance\n",
        "    try:\n",
        "        shap_values = explainer.shap_values(X_padded_kmer_ids[idx:idx+1])\n",
        "        # Rest of your explanation code...\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to explain sequence {idx}: {str(e)}\")\n",
        "\n",
        "    # Get the base value (expected value) - different for PermutationExplainer\n",
        "    if hasattr(explainer, 'expected_value'):\n",
        "        base_value = explainer.expected_value[1]  # For class 1\n",
        "    else:\n",
        "        # For PermutationExplainer, we need to calculate it differently\n",
        "        base_value = np.mean([shap_predict_ids(X_padded_kmer_ids[idx:idx+1])[0][1] for _ in range(10)])\n",
        "\n",
        "    # Extract SHAP values for class 1\n",
        "    if hasattr(shap_values, 'values'):\n",
        "        # Newer SHAP versions\n",
        "        shap_values_instance = shap_values.values[0, :, 1]\n",
        "    else:\n",
        "        # Older SHAP versions\n",
        "        shap_values_instance = shap_values[0, :, 1]\n",
        "\n",
        "    # Get the actual k-mer strings for this padded sequence\n",
        "    kmer_strings_for_viz = [id_to_kmer[id] for id in X_padded_kmer_ids[idx, :]]\n",
        "\n",
        "    # Create a SHAP Explanation object for text visualization\n",
        "    text_exp = shap.Explanation(\n",
        "        values=shap_values_instance,\n",
        "        base_values=base_value,  # Use our calculated base value\n",
        "        data=kmer_strings_for_viz,\n",
        "        feature_names=[str(i) for i in range(max_kmer_len)]\n",
        "    )\n",
        "\n",
        "    # Visualize\n",
        "    shap.plots.text(text_exp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqT1vaTxUZr3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import shap\n",
        "from collections import defaultdict\n",
        "\n",
        "# Function to tokenize peptide sequences into k-mers\n",
        "def seq_to_kmers(seq, k=3):\n",
        "    return ' '.join([seq[i:i+k] for i in range(len(seq) - k + 1)])\n",
        "\n",
        "k = 3\n",
        "df['kmers'] = df['sequence'].apply(lambda seq: seq_to_kmers(seq, k))\n",
        "kmer_texts = df['kmers'].tolist()\n",
        "\n",
        "# Function to convert k-mer text back to full sequence\n",
        "def kmers_to_seq(kmer_str, k=3):\n",
        "    kmers = kmer_str.split()\n",
        "    if not kmers:\n",
        "        return ''\n",
        "    return kmers[0] + ''.join([kmer[-1] for kmer in kmers[1:]])\n",
        "\n",
        "# 1. Build k-mer vocabulary and mapping to integer IDs\n",
        "all_kmers = [kmer for text in kmer_texts for kmer in text.split()]\n",
        "kmer_vocab = sorted(list(set(all_kmers)))\n",
        "# Add a padding token to the vocabulary\n",
        "padding_token = \"<PAD>\"\n",
        "if padding_token not in kmer_vocab:\n",
        "    kmer_vocab.append(padding_token)\n",
        "kmer_to_id = {kmer: i for i, kmer in enumerate(kmer_vocab)}\n",
        "id_to_kmer = {i: kmer for kmer, i in kmer_to_id.items()}\n",
        "padding_id = kmer_to_id[padding_token]\n",
        "\n",
        "\n",
        "# 2. Convert k-mer text sequences to integer ID sequences\n",
        "kmer_id_sequences = []\n",
        "for text in kmer_texts:\n",
        "    kmer_ids = [kmer_to_id[kmer] for kmer in text.split()]\n",
        "    kmer_id_sequences.append(kmer_ids)\n",
        "\n",
        "# Determine maximum k-mer sequence length for padding\n",
        "max_kmer_len = max(len(ids) for ids in kmer_id_sequences)\n",
        "# After defining max_kmer_len, add a check:\n",
        "print(f\"Maximum k-mer sequence length: {max_kmer_len}\")\n",
        "if max_kmer_len > 53:  # Or whatever your limit is\n",
        "    max_kmer_len = 53  # Set a safe upper limit\n",
        "    print(f\"Adjusted max k-mer sequence length to: {max_kmer_len}\")\n",
        "\n",
        "# Then pad with this adjusted length\n",
        "X_padded_kmer_ids = np.full((len(kmer_id_sequences), max_kmer_len), padding_id, dtype=int)\n",
        "for i, ids in enumerate(kmer_id_sequences):\n",
        "    length_to_copy = min(len(ids), max_kmer_len)\n",
        "    X_padded_kmer_ids[i, :length_to_copy] = ids[:length_to_copy]\n",
        "\n",
        "\n",
        "# 3. Pad integer ID sequences\n",
        "X_padded_kmer_ids = np.full((len(kmer_id_sequences), max_kmer_len), padding_id, dtype=int)\n",
        "for i, ids in enumerate(kmer_id_sequences):\n",
        "    X_padded_kmer_ids[i, :len(ids)] = ids\n",
        "\n",
        "\n",
        "# Define the missing predict_fn function\n",
        "def predict_fn(sequences):\n",
        "    \"\"\"\n",
        "    Takes list of sequences and returns predicted probabilities as numpy array (n_samples, 2)\n",
        "    Handles both ESM embeddings and one-hot encoded sequences\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Generate ESM embeddings\n",
        "        esm_feats = extract_esm_embeddings(sequences)  # Shape: [batch_size, esm_dim]\n",
        "\n",
        "        # Generate one-hot encoded sequences\n",
        "        onehot_seqs = sequence_to_onehot(sequences).float()  # Shape: [batch_size, seq_len, 4]\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = model(esm_feats, onehot_seqs)  # Now passing both required arguments\n",
        "        probs = torch.softmax(outputs, dim=1).numpy()\n",
        "\n",
        "    return probs  # Shape (n_samples, 2)\n",
        "\n",
        "\n",
        "# Now the shap_predict_ids function will work since predict_fn is defined\n",
        "def shap_predict_ids(padded_kmer_id_arrays):\n",
        "    original_seqs = []\n",
        "    for padded_row_ids in padded_kmer_id_arrays:\n",
        "        kmers_without_padding = [id_to_kmer[id] for id in padded_row_ids if id != padding_id]\n",
        "        kmer_text_str = ' '.join(kmers_without_padding)\n",
        "        original_seqs.append(kmers_to_seq(kmer_text_str, k))\n",
        "    return predict_fn(original_seqs)\n",
        "\n",
        "\n",
        "# Fix the labels reference (was using 'labels' which wasn't defined)\n",
        "labels = df['label'].tolist()\n",
        "sequences = df['sequence'].tolist()\n",
        "\n",
        "# Select high-confidence toxic predictions (using the now-defined predict_fn)\n",
        "probs = predict_fn(sequences)\n",
        "high_conf_ids = np.where((np.array(labels) == 0) & (probs[:, 1] > 0.8))[0]\n",
        "selected_ids = high_conf_ids[:10]\n",
        "\n",
        "# Explain each selected instance\n",
        "\n",
        "for idx in selected_ids:\n",
        "    if idx >= len(X_padded_kmer_ids):\n",
        "        continue  # Skip if index is out of bounds\n",
        "    print(f\"Explaining sequence ID: {idx}\")\n",
        "    # Get the explanation for this instance\n",
        "    try:\n",
        "        shap_values = explainer.shap_values(X_padded_kmer_ids[idx:idx+1])\n",
        "        # Rest of your explanation code...\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to explain sequence {idx}: {str(e)}\")\n",
        "\n",
        "    # Get the base value (expected value) - different for PermutationExplainer\n",
        "    if hasattr(explainer, 'expected_value'):\n",
        "        base_value = explainer.expected_value[0]  # For class 0\n",
        "    else:\n",
        "        # For PermutationExplainer, we need to calculate it differently\n",
        "        base_value = np.mean([shap_predict_ids(X_padded_kmer_ids[idx:idx+1])[0][1] for _ in range(10)])\n",
        "\n",
        "    # Extract SHAP values for class 1\n",
        "    if hasattr(shap_values, 'values'):\n",
        "        # Newer SHAP versions\n",
        "        shap_values_instance = shap_values.values[0, :, 0]\n",
        "    else:\n",
        "        # Older SHAP versions\n",
        "        shap_values_instance = shap_values[0, :, 0]\n",
        "\n",
        "    # Get the actual k-mer strings for this padded sequence\n",
        "    kmer_strings_for_viz = [id_to_kmer[id] for id in X_padded_kmer_ids[idx, :]]\n",
        "\n",
        "    # Create a SHAP Explanation object for text visualization\n",
        "    text_exp = shap.Explanation(\n",
        "        values=shap_values_instance,\n",
        "        base_values=base_value,  # Use our calculated base value\n",
        "        data=kmer_strings_for_viz,\n",
        "        feature_names=[str(i) for i in range(max_kmer_len)]\n",
        "    )\n",
        "\n",
        "    # Visualize\n",
        "    shap.plots.text(text_exp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRrywuXTUZr4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}